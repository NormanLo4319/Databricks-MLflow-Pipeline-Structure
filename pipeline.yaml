Description: >
  `pipeline.yaml` is the main configuration file for MLFlow Pipeline.
  Required pipeline parameters should be defined in this file with either concrete values or variables such as {{ INGEST_DATA_LOCATION }}

  Variables must be dereferenced in a profile YAML file, located under `profiles/`.
  See `profiles/local.yaml` for example usage. One may switch among profiles quickly by providing a profile name such as `local` in the Pipeline object constructor:
  `p = Pipeline(profile="local")`

  NOTE: YAML does not support tabs for indentation. Please use spaces and ensure that all YAML files are properly formatted.

template: "multi-family-rent/v1"

# Specifies the dataset to use for model development
data:
  # Define the ingest data location
  location: {{INGEST_DATA_LOCATION}}
  # Define the ingest data format, natively supported format includes `parquet`, `spark_sql`, and `delta`
  format: {{INGEST_DATA_FROMAT|defual('parquet')}}
  # Datasets with other formats, including `csv`, can be used by implementing and specifying a `custom_loader_method`
  custom_loader_method: steps.ingest.loadFileAsDataframe
  # If `spark_sql` format is specified, the `SQL` entry is used to specify a SparkSQL statement that identifies the dataset to use
  sql: SELECT * FROM nlo.`{{INGEST_DATA_LOCATION}}`

# Specify the dataset to use for batch scoring. All params serve the same function as in `data`
data_scoring:
  location: {{INGEST_SCORING_DATA_LOCATION}}
  format: {{INGEST_SCORING_DATA_FORMAT|default('parquet')}}
  custom_loader_method: steps.ingest.loadFileAsDataframe
  sql: SELECT * FROM nlo.`{{INGEST_SCORING_DATA_LOCATION}}`

# Specify the name of the column containing targets / labels for model training and evaluation
target_col: "rent"
steps:
  split:
    # Train/Validation/Test split ratios
    split_ratios: {{SPLIT_RATIOS|default([0.75, 0.125, 0.125])}}
    # Specifies the method to use to perform additional processing and cleaning on split datasets
    post_split_method: steps.split.processSplits
  transform:
    # Specifies the method that defines the data transformations to apply during model inference
    transformer_method: steps.transform.transformer_fn
  train:
    using: estimator_spec
    # Specifies the method that defines the estimator type and parameters to use for model training
    estimator_method: steps.train.estimator_fn
  evaluate:
    # Sets performance thresholds that a trained model must meet in order to be eligible for registration to the MLFlow Model Registry
    validation_criteria:
      - metric: root_mean_squared_error
        threshold: 500
      - metric: mean_absolute_error
        threshold: 700
      - metric: weighted_mean_squared_error
        threshold: 700
    register:
      # Specifies the name of the Registered Model
      model_name: "multi-family-rent-predictor"
      # Indicates whether or not a model that fails to meet performance thresholds should still be registered
      allow_non_validated_model: true
    predict:
      # Define the model URI to use in batch scoring here or use the latest model registered from the training DAG
      # model_uri: "models/model.pkl"
      # Specify the output path of the scored data from predict
      output_location: {{SCORE_OUTPUT_DATA_LOCATION}}
      # Specify the output format of the scored data from predict
      output_format: {{SCORED_OUTPUT_DATA_FORMAT|default('parquet')}}

metrics:
  # Defines custom performance metrics to compute during model training and evaluation
  custom:
    - name: weighted_mean_squared_error
      # Specifies the name of the function in `steps/custom-metrics.py` for computing the metric
      function: weighted_mean_squared_error
      greater_is_better: False
  # Sets the primary metric to evaluate model performance.
  # Primary metric is used to sort MLFlow Runs corresponding to the pipeline in the MLFlow Tracking UI
  primary: "root_mean_square_error"